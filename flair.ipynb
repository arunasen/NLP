{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunasen/NLP/blob/main/flair.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyJt1EBNclug"
      },
      "source": [
        "Install Flair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BSQQjIXzbrbk"
      },
      "outputs": [],
      "source": [
        "pip install flair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdy-vVeMcuPV"
      },
      "source": [
        "Approach 1: initial setup of character dictionary from corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8it8zphHgGs"
      },
      "outputs": [],
      "source": [
        "# make an empty character dictionary\n",
        "from flair.data import Dictionary\n",
        "char_dictionary: Dictionary = Dictionary()\n",
        "\n",
        "# counter object\n",
        "import collections\n",
        "counter = collections.Counter()\n",
        "\n",
        "processed = 0\n",
        "\n",
        "import glob\n",
        "files = glob.glob('/content/drive/MyDrive/corpus/*.*')\n",
        "\n",
        "print(files)\n",
        "for file in files:\n",
        "    print(file)\n",
        "\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        tokens = 0\n",
        "        for line in f:\n",
        "\n",
        "            processed += 1            \n",
        "            chars = list(line)\n",
        "            tokens += len(chars)\n",
        "\n",
        "            # Add chars to the dictionary\n",
        "            counter.update(chars)\n",
        "\n",
        "            # comment this line in to speed things up (if the corpus is too large)\n",
        "            # if tokens > 50000000: break\n",
        "\n",
        "    # break\n",
        "\n",
        "total_count = 0\n",
        "for letter, count in counter.most_common():\n",
        "    total_count += count\n",
        "\n",
        "print(total_count)\n",
        "print(processed)\n",
        "\n",
        "sum = 0\n",
        "idx = 0\n",
        "for letter, count in counter.most_common():\n",
        "    sum += count\n",
        "    percentile = (sum / total_count)\n",
        "\n",
        "    # comment this line in to use only top X percentile of chars, otherwise filter later\n",
        "    # if percentile < 0.00001: break\n",
        "\n",
        "    char_dictionary.add_item(letter)\n",
        "    idx += 1\n",
        "    print('%d\\t%s\\t%7d\\t%7d\\t%f' % (idx, letter, count, sum, percentile))\n",
        "\n",
        "print(char_dictionary.item2idx)\n",
        "\n",
        "import pickle\n",
        "with open('/content/drive/MyDrive/corpus/weiboEmbeddingTestRun', 'wb') as f:\n",
        "    mappings = {\n",
        "        'idx2item': char_dictionary.idx2item,\n",
        "        'item2idx': char_dictionary.item2idx\n",
        "    }\n",
        "    pickle.dump(mappings, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach 1: Use a portion of Weibo news only to train the model (experiement confirmed there is not enough data to adequately train a model)\n"
      ],
      "metadata": {
        "id": "o1-hfwSxKwSK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Qyv4lfsjbSjV"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from flair.data import Dictionary\n",
        "from flair.models import LanguageModel\n",
        "from flair.trainers.language_model_trainer import LanguageModelTrainer, TextCorpus\n",
        "\n",
        "# are you training a forward or backward LM?\n",
        "is_forward_lm = True\n",
        "\n",
        "# load the custom character dictionary\n",
        "dictionary = Dictionary.load_from_file('/content/drive/MyDrive/weiboEmbeddingTestRun')\n",
        "\n",
        "print(dictionary)\n",
        "\n",
        "# get your corpus, process forward and at the character level\n",
        "corpus = TextCorpus('/content/drive/MyDrive/corpus',dictionary,is_forward_lm, character_level=True)\n",
        "\n",
        "# instantiate your language model, set hidden size and number of layers\n",
        "language_model = LanguageModel(dictionary,\n",
        "                               is_forward_lm,\n",
        "                               hidden_size=128,\n",
        "                               nlayers=1)\n",
        "\n",
        "# train your language model\n",
        "trainer = LanguageModelTrainer(language_model, corpus)\n",
        "\n",
        "trainer.train('/content/drive/MyDrive/customWeibo',\n",
        "              sequence_length=10,\n",
        "              mini_batch_size=10,\n",
        "              max_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTKaCNM_dG4e"
      },
      "source": [
        "Approach 2: Set up custom embeddings and train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "toAAlG6NfJYu"
      },
      "outputs": [],
      "source": [
        "#using custom embeddings which are trained on sufficiently large datasets\n",
        "#from flair.data_fetcher import NLPTaskDataFetcher\n",
        "from flair.embeddings import TransformerWordEmbeddings, WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings, BytePairEmbeddings, CharacterEmbeddings\n",
        "from flair.models import TextClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "from flair.data import Corpus\n",
        "from flair.datasets import ClassificationCorpus\n",
        "data_folder = '/content/drive/MyDrive/data'\n",
        "corpus: Corpus = ClassificationCorpus(data_folder, test_file='testSegmented.csv',dev_file='devSegmented.csv',train_file='trainSegmented.csv')\n",
        "\n",
        "#Skipgram\n",
        "#merge = WordEmbeddings('/content/drive/MyDrive/customEmbedding/merge_sgns_bigram_char300.gensim')\n",
        "#weibo = WordEmbeddings('/content/drive/MyDrive/customEmbedding/sgns.weibo.bigram-char.gensim')\n",
        "#baidu = WordEmbeddings('/content/drive/MyDrive/customEmbedding/sgns.baidubaike.bigram-char.gensim')\n",
        "#literature = WordEmbeddings('/content/drive/MyDrive/customEmbedding/sgns.literature.bigram-char.gensim')\n",
        "#zhihu = WordEmbeddings('/content/drive/MyDrive/customEmbedding/sgns.zhihu.bigram-char.gensim')\n",
        "#wiki = WordEmbeddings('/content/drive/MyDrive/customEmbedding/sgns.wiki.bigram-char.gensim')\n",
        "#renmin = WordEmbeddings('/content/drive/MyDrive/customEmbedding/sgns.renmin.bigram-char.gensim')\n",
        "\n",
        "#GWE\n",
        "#weiboEmbeddings_s = WordEmbeddings('/content/drive/MyDrive/customEmbedding/gwe_chr_s.gensim')\n",
        "#weiboEmbeddings_b = WordEmbeddings('/content/drive/MyDrive/customEmbedding/gwe_chr_b.gensim')\n",
        "#weiboEmbeddings_m = WordEmbeddings('/content/drive/MyDrive/customEmbedding/gwe_chr_m.gensim')\n",
        "#weiboEmbeddings_e = WordEmbeddings('/content/drive/MyDrive/customEmbedding/gwe_chr_e.gensim')\n",
        "#weiboEmbeddings_v = WordEmbeddings('/content/drive/MyDrive/customEmbedding/gwe_vec.gensim')\n",
        "\n",
        "#JWE\n",
        "weiboEmbeddings_char = WordEmbeddings('/content/drive/MyDrive/customEmbedding/JWEchar_vec.gensim')\n",
        "weiboEmbeddings_comp = WordEmbeddings('/content/drive/MyDrive/customEmbedding/JWEcomp_vec.gensim')\n",
        "weiboEmbeddings_word = WordEmbeddings('/content/drive/MyDrive/customEmbedding/JWEword_vec.gensim')\n",
        "\n",
        "#RECWE\n",
        "#output_vec = WordEmbeddings('/content/drive/MyDrive/customEmbedding/output_vec.gensim')\n",
        "#char_vec = WordEmbeddings('/content/drive/MyDrive/customEmbedding/char_vec.gensim')\n",
        "#comp_vec = WordEmbeddings('/content/drive/MyDrive/customEmbedding/comp_vec.gensim')\n",
        "#output_char_vec = WordEmbeddings('/content/drive/MyDrive/customEmbedding/output_char_vec.gensim')\n",
        "\n",
        "#cw2vec\n",
        "#substoke_out = WordEmbeddings('/content/drive/MyDrive/customEmbedding/substoke_out.vec.gensim')\n",
        "#substoke_out_avg = WordEmbeddings('/content/drive/MyDrive/customEmbedding/substoke_out.avg.gensim')\n",
        "\n",
        "#CWE\n",
        "#word = WordEmbeddings('/content/drive/MyDrive/customEmbedding/word.gensim')\n",
        "#char_b = WordEmbeddings('/content/drive/MyDrive/customEmbedding/char_b.gensim')\n",
        "#char_s = WordEmbeddings('/content/drive/MyDrive/customEmbedding/char_s.gensim')\n",
        "#char_m = WordEmbeddings('/content/drive/MyDrive/customEmbedding/char_m.gensim')\n",
        "#char_e = WordEmbeddings('/content/drive/MyDrive/customEmbedding/char_e.gensim')\n",
        "\n",
        "#fasttext_wiki_embedding = WordEmbeddings('zh') #no good alone\n",
        "#fasttext_crawl_embedding = WordEmbeddings('zh-crawl') #no good alone\n",
        "#bert_embedding = TransformerWordEmbeddings('bert-base-chinese') #works well alone\n",
        "#byte_pair_embedding = BytePairEmbeddings('zh') #weak results alone\n",
        "#char_embeddings = CharacterEmbeddings()\n",
        "#custom_embeddings = FlairEmbeddings('/content/drive/MyDrive/customWeibo/best-lm.pt')\n",
        "\n",
        "word_embeddings = [weiboEmbeddings_word]#[fasttext_wiki_embedding, fasttext_crawl_embedding, bert_embedding, byte_pair_embedding]\n",
        "document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=1024)\n",
        "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
        "trainer = ModelTrainer(classifier, corpus)\n",
        "trainer.train('./', max_epochs=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach 3: TARS Classifier"
      ],
      "metadata": {
        "id": "JQ7wava3MgT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import flair\n",
        "from flair.data import Corpus\n",
        "from flair.datasets import ClassificationCorpus\n",
        "from flair.models.text_classification_model import TARSClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "#from flair.data_fetcher import NLPTaskDataFetcher\n",
        "#from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
        "#from flair.models import TextClassifier\n",
        "\n",
        "#from pathlib import Path\n",
        "#from flair.embeddings import TransformerWordEmbeddings\n",
        "#from flair.embeddings import BytePairEmbeddings\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#make a corpus\n",
        "data_folder = '/content/drive/MyDrive/data'\n",
        "corpus: Corpus = ClassificationCorpus(data_folder, test_file='test.csv',dev_file='dev.csv',train_file='train.csv')\n",
        "\n",
        "#Run TARS\n",
        "tars = TARSClassifier(task_name='tarsChinese', label_dictionary=corpus.make_label_dictionary())\n",
        "trainer = ModelTrainer(tars, corpus)\n",
        "trainer.train(base_path='balancedTARS', # path to store the model artifacts\n",
        "              learning_rate=0.02, # use very small learning rate\n",
        "              mini_batch_size=16,\n",
        "              mini_batch_chunk_size=4, # optionally set this if transformer is too much for your machine\n",
        "              max_epochs=10, # terminate after 10 epochs\n",
        "             )\n"
      ],
      "metadata": {
        "id": "WTGmW2p0MN_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get predictions at a sentence level for error analysis"
      ],
      "metadata": {
        "id": "xq-JbeCnK3Bf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFGivDGXolAQ"
      },
      "outputs": [],
      "source": [
        "from flair.data import Sentence\n",
        "\n",
        "csv = open('/content/drive/MyDrive/data/testSegmented.csv', newline='')\n",
        "model = TextClassifier.load('best-model.pt')\n",
        "predictions = list()\n",
        "for l in csv:\n",
        "   line = l.split('\\t')\n",
        "   sentence = Sentence(line[1])\n",
        "   print(sentence)\n",
        "   model.predict(sentence)\n",
        "   prediction = str(sentence.get_labels()[0]).split(' ')[0]\n",
        "   print(prediction)\n",
        "   if prediction == 'real':\n",
        "      predictions.append(1)\n",
        "   else:\n",
        "      predictions.append(0)\n",
        "print(predictions)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "flair.ipynb",
      "provenance": [],
      "mount_file_id": "1K-BFtf8e45AqaGmLc7CegXRmBxsLvCDU",
      "authorship_tag": "ABX9TyMpqbbrdtn1oWGr3x1c+zKt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}